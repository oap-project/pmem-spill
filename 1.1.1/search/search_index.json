{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PMem Spill Contents Introduction User Guide Introduction Spark supports to cache RDDs in memory and disk. We know that memory is small in size and costly. Disks are much slower, although with large capacity. PMem Spill supports RDD Cache with Intel Optane PMem, adds PMem storage level to the existing RDD cache solutions besides memory and disk. RDD Persistence One of the most important capabilities in Spark is persisting (or caching) a dataset across operations, each persisted RDD can be stored using a different storage level , allowing you, for example, to persist the dataset on disk, persist it in memory but as serialized Java objects (to save space), replicate it across nodes. These levels are set by passing a StorageLevel object, to persist() . The cache() method is a shorthand for using the default storage level, which is StorageLevel.MEMORY_ONLY (store deserialized objects in memory). The full set of storage levels is: Storage Level Meaning MEMORY_ONLY Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. This is the default level. MEMORY_AND_DISK Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed. MEMORY_ONLY_SER (Java and Scala) Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read. MEMORY_AND_DISK_SER (Java and Scala) Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed. DISK_ONLY Store the RDD partitions only on disk. MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc. Same as the levels above, but replicate each partition on two cluster nodes. OFF_HEAP (experimental) Similar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to be enabled. PMEM_ONLY Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to Intel Optane PMem instead of recomputing them on the fly each time they're needed. PMEM_AND_DISK Similar to MEMORY_AND_DISK_SER, but spill partitions that don't fit in memory to Intel Optane PMem and disk instead of recomputing them on the fly each time they're needed. Which Storage Level to Choose? Spark's storage levels are meant to provide different trade-offs between memory usage and CPU efficiency. PMem storage level is added to support a new tier for storage level besides memory and disk. Using PMem library to access Optane PMem can help to avoid the overhead from disk. Large capacity and high I/O performance of PMem shows better performance than tied DRAM and disk solution under the same cost. User Guide Installation We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you needn't compile and install Memkind, and you can find compiled OAP jars in $HOME/miniconda2/envs/oapenv/oap_jars . Prerequisites The following are required to configure OAP to use PMem cache in AppDirect mode. - PMem hardware is successfully deployed on each node in cluster. - Directories exposing PMem hardware on each socket. For example, on a two socket system the mounted PMem directories should appear as /mnt/pmem0 and /mnt/pmem1 . Correctly installed PMem must be formatted and mounted on every cluster worker node. // use ipmctl command to show topology and dimm info of PMem ipmctl show -topology ipmctl show -dimm // provision PMem in app direct mode ipmctl create -goal PersistentMemoryType=AppDirect // reboot system to make configuration take affect reboot // check capacity provisioned for app direct mode(AppDirectCapacity) ipmctl show -memoryresources // show the PMem region information ipmctl show -region // create namespace based on the region, multi namespaces can be created on a single region ndctl create-namespace -m fsdax -r region0 ndctl create-namespace -m fsdax -r region1 // show the created namespaces fdisk -l // create and mount file system echo y | mkfs.ext4 /dev/pmem0 echo y | mkfs.ext4 /dev/pmem1 mkdir -p /mnt/pmem0 mkdir -p /mnt/pmem1 mount -o dax /dev/pmem0 /mnt/pmem0 mount -o dax /dev/pmem1 /mnt/pmem1 In this case file systems are generated for 2 numa nodes, which can be checked by \"numactl --hardware\". For a different number of numa nodes, a corresponding number of namespaces should be created to assure correct file system paths mapping to numa nodes. Make sure Memkind library installed on every cluster worker node. The Memkind library depends on libnuma at the runtime, so it must already exist in the worker node system. Build the latest memkind lib from source: git clone -b v1.10.1 https://github.com/memkind/memkind cd memkind ./autogen.sh ./configure make make install For KMem Dax mode, we need to configure PMem as system ram. Kernel 5.1 or above is required to this mode. daxctl migrate-device-model ndctl create-namespace --mode=devdax --map=mem ndctl list daxctl reconfigure-device dax0.0 --mode=system-ram daxctl reconfigure-device dax1.0 --mode=system-ram daxctl reconfigure-device daxX.Y --mode=system-ram Refer Memkind KMem for details. Compiling Before building PMem Spill, install PMem-Common locally: git clone -b <tag-version> https://github.com/oap-project/pmem-common.git cd pmem-common mvn clean install -DskipTests To build pmem spill, you can run below commands: cd ${PMEM-SPILL} mvn clean package -DskipTests You will find jar files under oap-common/target and oap-spark/target. Configuration To enable rdd cache on Intel Optane PMem, you need add the following configurations to spark-defaults.conf spark.memory.pmem.extension.enabled true spark.memory.pmem.initial.path [Your Optane PMem paths seperate with comma] spark.memory.pmem.initial.size [Your Optane PMem size in GB] spark.memory.pmem.usable.ratio [from 0 to 1, 0.85 is recommended] spark.yarn.numa.enabled true spark.yarn.numa.num [Your numa node number] spark.memory.pmem.mode [AppDirect | KMemDax] spark.files file://${PATH_TO_PMEM_SPILL_JAR}/pmem-rdd-cache-<version>-with-spark-<version>.jar,file://${PATH_TO_PMEM_COMMON_JAR}/pmem-common-<version>-with-spark-<version>.jar spark.executor.extraClassPath ./pmem-rdd-cache-<version>-with-spark-<version>.jar:./pmem-common-<version>-with-spark-<version>.jar spark.driver.extraClassPath file://${PATH_TO_PMEM_SPILL_JAR}/pmem-rdd-cache-<version>-with-spark-<version>.jar:file://${PATH_TO_PMEM_COMMON_JAR}/pmem-common-<version>-with-spark-<version>.jar Use Optane PMem to cache data There's a new StorageLevel: PMEM_AND_DISK being added to cache data to Optane PMem, at the places you previously cache/persist data to memory, use PMEM_AND_DISK to substitute the previous StorageLevel, data will be cached to Optane PMem. persist(StorageLevel.PMEM_AND_DISK) Run K-means benchmark You can use Hibench to run K-means workload: After you Build Hibench, then follow Run SparkBench documentation. Here are some tips besides this documentation you need to notice. Follow the documentation to configure these 4 files: HiBench/conf/hadoop.conf HiBench/conf/hibench.conf HiBench/conf/spark.conf HiBench/conf/workloads/ml/kmeans.conf Note that you need add hibench.kmeans.storage.level PMEM_AND_DISK to kmeans.conf , which can enable both PMem and Disk to cache data. If you completed OAP-Installation-Guide , you also need add the following configs to spark.conf spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib Then you can run the following 2 commands to run K-means workloads: bin/workloads/ml/kmeans/prepare/prepare.sh bin/workloads/ml/kmeans/spark/run.sh Then you can find the log as below: patching args= Parsing conf: /home/wh/HiBench/conf/hadoop.conf Parsing conf: /home/wh/HiBench/conf/hibench.conf Parsing conf: /home/wh/HiBench/conf/spark.conf Parsing conf: /home/wh/HiBench/conf/workloads/ml/kmeans.conf probe sleep jar: /opt/Beaver/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar start ScalaSparkKmeans bench hdfs rm -r: /opt/Beaver/hadoop/bin/hadoop --config /opt/Beaver/hadoop/etc/hadoop fs -rm -r -skipTrash hdfs://vsr219:9000/HiBench/Kmeans/Output rm: `hdfs://vsr219:9000/HiBench/Kmeans/Output': No such file or directory hdfs du -s: /opt/Beaver/hadoop/bin/hadoop --config /opt/Beaver/hadoop/etc/hadoop fs -du -s hdfs://vsr219:9000/HiBench/Kmeans/Input Export env: SPARKBENCH_PROPERTIES_FILES=/home/wh/HiBench/report/kmeans/spark/conf/sparkbench/sparkbench.conf Export env: HADOOP_CONF_DIR=/opt/Beaver/hadoop/etc/hadoop Submit Spark job: /opt/Beaver/spark/bin/spark-submit --properties-file /home/wh/HiBench/report/kmeans/spark/conf/sparkbench/spark.conf --class com.intel.hibench.sparkbench.ml.DenseKMeans --master yarn-client --num-executors 2 --executor-cores 45 --executor-memory 100g /home/wh/HiBench/sparkbench/assembly/target/sparkbench-assembly-8.0-SNAPSHOT-dist.jar -k 10 --numIterations 5 --storageLevel PMEM_AND_DISK hdfs://vsr219:9000/HiBench/Kmeans/Input/samples 20/07/03 09:07:49 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-43459116-f4e3-4fe1-bb29-9bca0afa5286 finish ScalaSparkKmeans bench Open the Spark History Web UI and go to the Storage tab page to verify the cache metrics. Limitations For the scenario that data will exceed the block cache capacity. Memkind 1.9.0 and kernel 4.18 is recommended to avoid the unexpected issue. How to contribute Currently, PMem Spill packages includes all Spark changed files. MemoryMode.java MemoryManager.scala StorageMemoryPool.scala UnifiedMemoryManager.scala MemoryStore.scala BlockManager.scala StorageLevel.scala TestMemoryManager Please make sure your code change in above source code will not break current function. The files from this package should avoid depending on other OAP module except PMem-Common.","title":"PMem Spill"},{"location":"#pmem-spill","text":"","title":"PMem Spill"},{"location":"#contents","text":"Introduction User Guide","title":"Contents"},{"location":"#introduction","text":"Spark supports to cache RDDs in memory and disk. We know that memory is small in size and costly. Disks are much slower, although with large capacity. PMem Spill supports RDD Cache with Intel Optane PMem, adds PMem storage level to the existing RDD cache solutions besides memory and disk.","title":"Introduction"},{"location":"#rdd-persistence","text":"One of the most important capabilities in Spark is persisting (or caching) a dataset across operations, each persisted RDD can be stored using a different storage level , allowing you, for example, to persist the dataset on disk, persist it in memory but as serialized Java objects (to save space), replicate it across nodes. These levels are set by passing a StorageLevel object, to persist() . The cache() method is a shorthand for using the default storage level, which is StorageLevel.MEMORY_ONLY (store deserialized objects in memory). The full set of storage levels is: Storage Level Meaning MEMORY_ONLY Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. This is the default level. MEMORY_AND_DISK Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed. MEMORY_ONLY_SER (Java and Scala) Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read. MEMORY_AND_DISK_SER (Java and Scala) Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed. DISK_ONLY Store the RDD partitions only on disk. MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc. Same as the levels above, but replicate each partition on two cluster nodes. OFF_HEAP (experimental) Similar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to be enabled. PMEM_ONLY Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to Intel Optane PMem instead of recomputing them on the fly each time they're needed. PMEM_AND_DISK Similar to MEMORY_AND_DISK_SER, but spill partitions that don't fit in memory to Intel Optane PMem and disk instead of recomputing them on the fly each time they're needed.","title":"RDD Persistence"},{"location":"#which-storage-level-to-choose","text":"Spark's storage levels are meant to provide different trade-offs between memory usage and CPU efficiency. PMem storage level is added to support a new tier for storage level besides memory and disk. Using PMem library to access Optane PMem can help to avoid the overhead from disk. Large capacity and high I/O performance of PMem shows better performance than tied DRAM and disk solution under the same cost.","title":"Which Storage Level to Choose?"},{"location":"#user-guide","text":"","title":"User Guide"},{"location":"#installation","text":"We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you needn't compile and install Memkind, and you can find compiled OAP jars in $HOME/miniconda2/envs/oapenv/oap_jars .","title":"Installation"},{"location":"#prerequisites","text":"The following are required to configure OAP to use PMem cache in AppDirect mode. - PMem hardware is successfully deployed on each node in cluster. - Directories exposing PMem hardware on each socket. For example, on a two socket system the mounted PMem directories should appear as /mnt/pmem0 and /mnt/pmem1 . Correctly installed PMem must be formatted and mounted on every cluster worker node. // use ipmctl command to show topology and dimm info of PMem ipmctl show -topology ipmctl show -dimm // provision PMem in app direct mode ipmctl create -goal PersistentMemoryType=AppDirect // reboot system to make configuration take affect reboot // check capacity provisioned for app direct mode(AppDirectCapacity) ipmctl show -memoryresources // show the PMem region information ipmctl show -region // create namespace based on the region, multi namespaces can be created on a single region ndctl create-namespace -m fsdax -r region0 ndctl create-namespace -m fsdax -r region1 // show the created namespaces fdisk -l // create and mount file system echo y | mkfs.ext4 /dev/pmem0 echo y | mkfs.ext4 /dev/pmem1 mkdir -p /mnt/pmem0 mkdir -p /mnt/pmem1 mount -o dax /dev/pmem0 /mnt/pmem0 mount -o dax /dev/pmem1 /mnt/pmem1 In this case file systems are generated for 2 numa nodes, which can be checked by \"numactl --hardware\". For a different number of numa nodes, a corresponding number of namespaces should be created to assure correct file system paths mapping to numa nodes. Make sure Memkind library installed on every cluster worker node. The Memkind library depends on libnuma at the runtime, so it must already exist in the worker node system. Build the latest memkind lib from source: git clone -b v1.10.1 https://github.com/memkind/memkind cd memkind ./autogen.sh ./configure make make install For KMem Dax mode, we need to configure PMem as system ram. Kernel 5.1 or above is required to this mode. daxctl migrate-device-model ndctl create-namespace --mode=devdax --map=mem ndctl list daxctl reconfigure-device dax0.0 --mode=system-ram daxctl reconfigure-device dax1.0 --mode=system-ram daxctl reconfigure-device daxX.Y --mode=system-ram Refer Memkind KMem for details.","title":"Prerequisites"},{"location":"#compiling","text":"Before building PMem Spill, install PMem-Common locally: git clone -b <tag-version> https://github.com/oap-project/pmem-common.git cd pmem-common mvn clean install -DskipTests To build pmem spill, you can run below commands: cd ${PMEM-SPILL} mvn clean package -DskipTests You will find jar files under oap-common/target and oap-spark/target.","title":"Compiling"},{"location":"#configuration","text":"To enable rdd cache on Intel Optane PMem, you need add the following configurations to spark-defaults.conf spark.memory.pmem.extension.enabled true spark.memory.pmem.initial.path [Your Optane PMem paths seperate with comma] spark.memory.pmem.initial.size [Your Optane PMem size in GB] spark.memory.pmem.usable.ratio [from 0 to 1, 0.85 is recommended] spark.yarn.numa.enabled true spark.yarn.numa.num [Your numa node number] spark.memory.pmem.mode [AppDirect | KMemDax] spark.files file://${PATH_TO_PMEM_SPILL_JAR}/pmem-rdd-cache-<version>-with-spark-<version>.jar,file://${PATH_TO_PMEM_COMMON_JAR}/pmem-common-<version>-with-spark-<version>.jar spark.executor.extraClassPath ./pmem-rdd-cache-<version>-with-spark-<version>.jar:./pmem-common-<version>-with-spark-<version>.jar spark.driver.extraClassPath file://${PATH_TO_PMEM_SPILL_JAR}/pmem-rdd-cache-<version>-with-spark-<version>.jar:file://${PATH_TO_PMEM_COMMON_JAR}/pmem-common-<version>-with-spark-<version>.jar","title":"Configuration"},{"location":"#use-optane-pmem-to-cache-data","text":"There's a new StorageLevel: PMEM_AND_DISK being added to cache data to Optane PMem, at the places you previously cache/persist data to memory, use PMEM_AND_DISK to substitute the previous StorageLevel, data will be cached to Optane PMem. persist(StorageLevel.PMEM_AND_DISK)","title":"Use Optane PMem to cache data"},{"location":"#run-k-means-benchmark","text":"You can use Hibench to run K-means workload: After you Build Hibench, then follow Run SparkBench documentation. Here are some tips besides this documentation you need to notice. Follow the documentation to configure these 4 files: HiBench/conf/hadoop.conf HiBench/conf/hibench.conf HiBench/conf/spark.conf HiBench/conf/workloads/ml/kmeans.conf Note that you need add hibench.kmeans.storage.level PMEM_AND_DISK to kmeans.conf , which can enable both PMem and Disk to cache data. If you completed OAP-Installation-Guide , you also need add the following configs to spark.conf spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib Then you can run the following 2 commands to run K-means workloads: bin/workloads/ml/kmeans/prepare/prepare.sh bin/workloads/ml/kmeans/spark/run.sh Then you can find the log as below: patching args= Parsing conf: /home/wh/HiBench/conf/hadoop.conf Parsing conf: /home/wh/HiBench/conf/hibench.conf Parsing conf: /home/wh/HiBench/conf/spark.conf Parsing conf: /home/wh/HiBench/conf/workloads/ml/kmeans.conf probe sleep jar: /opt/Beaver/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar start ScalaSparkKmeans bench hdfs rm -r: /opt/Beaver/hadoop/bin/hadoop --config /opt/Beaver/hadoop/etc/hadoop fs -rm -r -skipTrash hdfs://vsr219:9000/HiBench/Kmeans/Output rm: `hdfs://vsr219:9000/HiBench/Kmeans/Output': No such file or directory hdfs du -s: /opt/Beaver/hadoop/bin/hadoop --config /opt/Beaver/hadoop/etc/hadoop fs -du -s hdfs://vsr219:9000/HiBench/Kmeans/Input Export env: SPARKBENCH_PROPERTIES_FILES=/home/wh/HiBench/report/kmeans/spark/conf/sparkbench/sparkbench.conf Export env: HADOOP_CONF_DIR=/opt/Beaver/hadoop/etc/hadoop Submit Spark job: /opt/Beaver/spark/bin/spark-submit --properties-file /home/wh/HiBench/report/kmeans/spark/conf/sparkbench/spark.conf --class com.intel.hibench.sparkbench.ml.DenseKMeans --master yarn-client --num-executors 2 --executor-cores 45 --executor-memory 100g /home/wh/HiBench/sparkbench/assembly/target/sparkbench-assembly-8.0-SNAPSHOT-dist.jar -k 10 --numIterations 5 --storageLevel PMEM_AND_DISK hdfs://vsr219:9000/HiBench/Kmeans/Input/samples 20/07/03 09:07:49 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-43459116-f4e3-4fe1-bb29-9bca0afa5286 finish ScalaSparkKmeans bench Open the Spark History Web UI and go to the Storage tab page to verify the cache metrics.","title":"Run K-means benchmark"},{"location":"#limitations","text":"For the scenario that data will exceed the block cache capacity. Memkind 1.9.0 and kernel 4.18 is recommended to avoid the unexpected issue.","title":"Limitations"},{"location":"#how-to-contribute","text":"Currently, PMem Spill packages includes all Spark changed files. MemoryMode.java MemoryManager.scala StorageMemoryPool.scala UnifiedMemoryManager.scala MemoryStore.scala BlockManager.scala StorageLevel.scala TestMemoryManager Please make sure your code change in above source code will not break current function. The files from this package should avoid depending on other OAP module except PMem-Common.","title":"How to contribute"},{"location":"OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP modules. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Native SQL Engine Building OAP Prerequisites We provide scripts to help automatically install dependencies required, please change to root user and run: # git clone -b <tag-version> https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Note : oap-tools tag version v1.1.1-spark-3.1.1 corresponds to all OAP modules' tag version v1.1.1-spark-3.1.1 . Then the dependencies below will be installed: Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow LLVM Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. Building OAP is built with Apache Maven and Oracle Java 8. To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache Enabling NUMA binding for PMem in Spark Rebuilding Spark packages with NUMA binding patch When using PMem as a cache medium apply the NUMA binding patch numa-binding-spark-3.1.1.patch to Spark source code for best performance. Download src for Spark-3.1.1 and clone the src from github. Apply this patch and rebuild the Spark package. git apply numa-binding-spark-3.1.1.patch Add these configuration items to the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf to enable NUMA binding. spark.yarn.numa.enabled true","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP modules. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Native SQL Engine","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"OAP-Developer-Guide/#prerequisites","text":"We provide scripts to help automatically install dependencies required, please change to root user and run: # git clone -b <tag-version> https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Note : oap-tools tag version v1.1.1-spark-3.1.1 corresponds to all OAP modules' tag version v1.1.1-spark-3.1.1 . Then the dependencies below will be installed: Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow LLVM Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance.","title":"Prerequisites"},{"location":"OAP-Developer-Guide/#building","text":"OAP is built with Apache Maven and Oracle Java 8. To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"Building"},{"location":"OAP-Developer-Guide/#enabling-numa-binding-for-pmem-in-spark","text":"","title":"Enabling NUMA binding for PMem in Spark"},{"location":"OAP-Developer-Guide/#rebuilding-spark-packages-with-numa-binding-patch","text":"When using PMem as a cache medium apply the NUMA binding patch numa-binding-spark-3.1.1.patch to Spark source code for best performance. Download src for Spark-3.1.1 and clone the src from github. Apply this patch and rebuild the Spark package. git apply numa-binding-spark-3.1.1.patch Add these configuration items to the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf to enable NUMA binding. spark.yarn.numa.enabled true","title":"Rebuilding Spark packages with NUMA binding patch"},{"location":"OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Create a Conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.1.1 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar Then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"OAP-Installation-Guide/#installing-oap","text":"Create a Conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.1.1 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI","title":"Installing OAP"},{"location":"OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar Then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"User-Guide/","text":"PMem Spill Contents Introduction User Guide Introduction Spark supports to cache RDDs in memory and disk. We know that memory is small in size and costly. Disks are much slower, although with large capacity. PMem Spill supports RDD Cache with Intel Optane PMem, adds PMem storage level to the existing RDD cache solutions besides memory and disk. RDD Persistence One of the most important capabilities in Spark is persisting (or caching) a dataset across operations, each persisted RDD can be stored using a different storage level , allowing you, for example, to persist the dataset on disk, persist it in memory but as serialized Java objects (to save space), replicate it across nodes. These levels are set by passing a StorageLevel object, to persist() . The cache() method is a shorthand for using the default storage level, which is StorageLevel.MEMORY_ONLY (store deserialized objects in memory). The full set of storage levels is: Storage Level Meaning MEMORY_ONLY Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. This is the default level. MEMORY_AND_DISK Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed. MEMORY_ONLY_SER (Java and Scala) Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read. MEMORY_AND_DISK_SER (Java and Scala) Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed. DISK_ONLY Store the RDD partitions only on disk. MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc. Same as the levels above, but replicate each partition on two cluster nodes. OFF_HEAP (experimental) Similar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to be enabled. PMEM_ONLY Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to Intel Optane PMem instead of recomputing them on the fly each time they're needed. PMEM_AND_DISK Similar to MEMORY_AND_DISK_SER, but spill partitions that don't fit in memory to Intel Optane PMem and disk instead of recomputing them on the fly each time they're needed. Which Storage Level to Choose? Spark's storage levels are meant to provide different trade-offs between memory usage and CPU efficiency. PMem storage level is added to support a new tier for storage level besides memory and disk. Using PMem library to access Optane PMem can help to avoid the overhead from disk. Large capacity and high I/O performance of PMem shows better performance than tied DRAM and disk solution under the same cost. User Guide Installation We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you needn't compile and install Memkind, and you can find compiled OAP jars in $HOME/miniconda2/envs/oapenv/oap_jars . Prerequisites The following are required to configure OAP to use PMem cache in AppDirect mode. - PMem hardware is successfully deployed on each node in cluster. - Directories exposing PMem hardware on each socket. For example, on a two socket system the mounted PMem directories should appear as /mnt/pmem0 and /mnt/pmem1 . Correctly installed PMem must be formatted and mounted on every cluster worker node. // use ipmctl command to show topology and dimm info of PMem ipmctl show -topology ipmctl show -dimm // provision PMem in app direct mode ipmctl create -goal PersistentMemoryType=AppDirect // reboot system to make configuration take affect reboot // check capacity provisioned for app direct mode(AppDirectCapacity) ipmctl show -memoryresources // show the PMem region information ipmctl show -region // create namespace based on the region, multi namespaces can be created on a single region ndctl create-namespace -m fsdax -r region0 ndctl create-namespace -m fsdax -r region1 // show the created namespaces fdisk -l // create and mount file system echo y | mkfs.ext4 /dev/pmem0 echo y | mkfs.ext4 /dev/pmem1 mkdir -p /mnt/pmem0 mkdir -p /mnt/pmem1 mount -o dax /dev/pmem0 /mnt/pmem0 mount -o dax /dev/pmem1 /mnt/pmem1 In this case file systems are generated for 2 numa nodes, which can be checked by \"numactl --hardware\". For a different number of numa nodes, a corresponding number of namespaces should be created to assure correct file system paths mapping to numa nodes. Make sure Memkind library installed on every cluster worker node. The Memkind library depends on libnuma at the runtime, so it must already exist in the worker node system. Build the latest memkind lib from source: git clone -b v1.10.1 https://github.com/memkind/memkind cd memkind ./autogen.sh ./configure make make install For KMem Dax mode, we need to configure PMem as system ram. Kernel 5.1 or above is required to this mode. daxctl migrate-device-model ndctl create-namespace --mode=devdax --map=mem ndctl list daxctl reconfigure-device dax0.0 --mode=system-ram daxctl reconfigure-device dax1.0 --mode=system-ram daxctl reconfigure-device daxX.Y --mode=system-ram Refer Memkind KMem for details. Compiling Before building PMem Spill, install PMem-Common locally: git clone -b <tag-version> https://github.com/oap-project/pmem-common.git cd pmem-common mvn clean install -DskipTests To build pmem spill, you can run below commands: cd ${PMEM-SPILL} mvn clean package -DskipTests You will find jar files under oap-common/target and oap-spark/target. Configuration To enable rdd cache on Intel Optane PMem, you need add the following configurations to spark-defaults.conf spark.memory.pmem.extension.enabled true spark.memory.pmem.initial.path [Your Optane PMem paths seperate with comma] spark.memory.pmem.initial.size [Your Optane PMem size in GB] spark.memory.pmem.usable.ratio [from 0 to 1, 0.85 is recommended] spark.yarn.numa.enabled true spark.yarn.numa.num [Your numa node number] spark.memory.pmem.mode [AppDirect | KMemDax] spark.files file://${PATH_TO_PMEM_SPILL_JAR}/pmem-rdd-cache-<version>-with-spark-<version>.jar,file://${PATH_TO_PMEM_COMMON_JAR}/pmem-common-<version>-with-spark-<version>.jar spark.executor.extraClassPath ./pmem-rdd-cache-<version>-with-spark-<version>.jar:./pmem-common-<version>-with-spark-<version>.jar spark.driver.extraClassPath file://${PATH_TO_PMEM_SPILL_JAR}/pmem-rdd-cache-<version>-with-spark-<version>.jar:file://${PATH_TO_PMEM_COMMON_JAR}/pmem-common-<version>-with-spark-<version>.jar Use Optane PMem to cache data There's a new StorageLevel: PMEM_AND_DISK being added to cache data to Optane PMem, at the places you previously cache/persist data to memory, use PMEM_AND_DISK to substitute the previous StorageLevel, data will be cached to Optane PMem. persist(StorageLevel.PMEM_AND_DISK) Run K-means benchmark You can use Hibench to run K-means workload: After you Build Hibench, then follow Run SparkBench documentation. Here are some tips besides this documentation you need to notice. Follow the documentation to configure these 4 files: HiBench/conf/hadoop.conf HiBench/conf/hibench.conf HiBench/conf/spark.conf HiBench/conf/workloads/ml/kmeans.conf Note that you need add hibench.kmeans.storage.level PMEM_AND_DISK to kmeans.conf , which can enable both PMem and Disk to cache data. If you completed OAP-Installation-Guide , you also need add the following configs to spark.conf spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib Then you can run the following 2 commands to run K-means workloads: bin/workloads/ml/kmeans/prepare/prepare.sh bin/workloads/ml/kmeans/spark/run.sh Then you can find the log as below: patching args= Parsing conf: /home/wh/HiBench/conf/hadoop.conf Parsing conf: /home/wh/HiBench/conf/hibench.conf Parsing conf: /home/wh/HiBench/conf/spark.conf Parsing conf: /home/wh/HiBench/conf/workloads/ml/kmeans.conf probe sleep jar: /opt/Beaver/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar start ScalaSparkKmeans bench hdfs rm -r: /opt/Beaver/hadoop/bin/hadoop --config /opt/Beaver/hadoop/etc/hadoop fs -rm -r -skipTrash hdfs://vsr219:9000/HiBench/Kmeans/Output rm: `hdfs://vsr219:9000/HiBench/Kmeans/Output': No such file or directory hdfs du -s: /opt/Beaver/hadoop/bin/hadoop --config /opt/Beaver/hadoop/etc/hadoop fs -du -s hdfs://vsr219:9000/HiBench/Kmeans/Input Export env: SPARKBENCH_PROPERTIES_FILES=/home/wh/HiBench/report/kmeans/spark/conf/sparkbench/sparkbench.conf Export env: HADOOP_CONF_DIR=/opt/Beaver/hadoop/etc/hadoop Submit Spark job: /opt/Beaver/spark/bin/spark-submit --properties-file /home/wh/HiBench/report/kmeans/spark/conf/sparkbench/spark.conf --class com.intel.hibench.sparkbench.ml.DenseKMeans --master yarn-client --num-executors 2 --executor-cores 45 --executor-memory 100g /home/wh/HiBench/sparkbench/assembly/target/sparkbench-assembly-8.0-SNAPSHOT-dist.jar -k 10 --numIterations 5 --storageLevel PMEM_AND_DISK hdfs://vsr219:9000/HiBench/Kmeans/Input/samples 20/07/03 09:07:49 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-43459116-f4e3-4fe1-bb29-9bca0afa5286 finish ScalaSparkKmeans bench Open the Spark History Web UI and go to the Storage tab page to verify the cache metrics. Limitations For the scenario that data will exceed the block cache capacity. Memkind 1.9.0 and kernel 4.18 is recommended to avoid the unexpected issue. How to contribute Currently, PMem Spill packages includes all Spark changed files. MemoryMode.java MemoryManager.scala StorageMemoryPool.scala UnifiedMemoryManager.scala MemoryStore.scala BlockManager.scala StorageLevel.scala TestMemoryManager Please make sure your code change in above source code will not break current function. The files from this package should avoid depending on other OAP module except PMem-Common.","title":"User Guide"},{"location":"User-Guide/#pmem-spill","text":"","title":"PMem Spill"},{"location":"User-Guide/#contents","text":"Introduction User Guide","title":"Contents"},{"location":"User-Guide/#introduction","text":"Spark supports to cache RDDs in memory and disk. We know that memory is small in size and costly. Disks are much slower, although with large capacity. PMem Spill supports RDD Cache with Intel Optane PMem, adds PMem storage level to the existing RDD cache solutions besides memory and disk.","title":"Introduction"},{"location":"User-Guide/#rdd-persistence","text":"One of the most important capabilities in Spark is persisting (or caching) a dataset across operations, each persisted RDD can be stored using a different storage level , allowing you, for example, to persist the dataset on disk, persist it in memory but as serialized Java objects (to save space), replicate it across nodes. These levels are set by passing a StorageLevel object, to persist() . The cache() method is a shorthand for using the default storage level, which is StorageLevel.MEMORY_ONLY (store deserialized objects in memory). The full set of storage levels is: Storage Level Meaning MEMORY_ONLY Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. This is the default level. MEMORY_AND_DISK Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed. MEMORY_ONLY_SER (Java and Scala) Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read. MEMORY_AND_DISK_SER (Java and Scala) Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed. DISK_ONLY Store the RDD partitions only on disk. MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc. Same as the levels above, but replicate each partition on two cluster nodes. OFF_HEAP (experimental) Similar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to be enabled. PMEM_ONLY Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to Intel Optane PMem instead of recomputing them on the fly each time they're needed. PMEM_AND_DISK Similar to MEMORY_AND_DISK_SER, but spill partitions that don't fit in memory to Intel Optane PMem and disk instead of recomputing them on the fly each time they're needed.","title":"RDD Persistence"},{"location":"User-Guide/#which-storage-level-to-choose","text":"Spark's storage levels are meant to provide different trade-offs between memory usage and CPU efficiency. PMem storage level is added to support a new tier for storage level besides memory and disk. Using PMem library to access Optane PMem can help to avoid the overhead from disk. Large capacity and high I/O performance of PMem shows better performance than tied DRAM and disk solution under the same cost.","title":"Which Storage Level to Choose?"},{"location":"User-Guide/#user-guide","text":"","title":"User Guide"},{"location":"User-Guide/#installation","text":"We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you needn't compile and install Memkind, and you can find compiled OAP jars in $HOME/miniconda2/envs/oapenv/oap_jars .","title":"Installation"},{"location":"User-Guide/#prerequisites","text":"The following are required to configure OAP to use PMem cache in AppDirect mode. - PMem hardware is successfully deployed on each node in cluster. - Directories exposing PMem hardware on each socket. For example, on a two socket system the mounted PMem directories should appear as /mnt/pmem0 and /mnt/pmem1 . Correctly installed PMem must be formatted and mounted on every cluster worker node. // use ipmctl command to show topology and dimm info of PMem ipmctl show -topology ipmctl show -dimm // provision PMem in app direct mode ipmctl create -goal PersistentMemoryType=AppDirect // reboot system to make configuration take affect reboot // check capacity provisioned for app direct mode(AppDirectCapacity) ipmctl show -memoryresources // show the PMem region information ipmctl show -region // create namespace based on the region, multi namespaces can be created on a single region ndctl create-namespace -m fsdax -r region0 ndctl create-namespace -m fsdax -r region1 // show the created namespaces fdisk -l // create and mount file system echo y | mkfs.ext4 /dev/pmem0 echo y | mkfs.ext4 /dev/pmem1 mkdir -p /mnt/pmem0 mkdir -p /mnt/pmem1 mount -o dax /dev/pmem0 /mnt/pmem0 mount -o dax /dev/pmem1 /mnt/pmem1 In this case file systems are generated for 2 numa nodes, which can be checked by \"numactl --hardware\". For a different number of numa nodes, a corresponding number of namespaces should be created to assure correct file system paths mapping to numa nodes. Make sure Memkind library installed on every cluster worker node. The Memkind library depends on libnuma at the runtime, so it must already exist in the worker node system. Build the latest memkind lib from source: git clone -b v1.10.1 https://github.com/memkind/memkind cd memkind ./autogen.sh ./configure make make install For KMem Dax mode, we need to configure PMem as system ram. Kernel 5.1 or above is required to this mode. daxctl migrate-device-model ndctl create-namespace --mode=devdax --map=mem ndctl list daxctl reconfigure-device dax0.0 --mode=system-ram daxctl reconfigure-device dax1.0 --mode=system-ram daxctl reconfigure-device daxX.Y --mode=system-ram Refer Memkind KMem for details.","title":"Prerequisites"},{"location":"User-Guide/#compiling","text":"Before building PMem Spill, install PMem-Common locally: git clone -b <tag-version> https://github.com/oap-project/pmem-common.git cd pmem-common mvn clean install -DskipTests To build pmem spill, you can run below commands: cd ${PMEM-SPILL} mvn clean package -DskipTests You will find jar files under oap-common/target and oap-spark/target.","title":"Compiling"},{"location":"User-Guide/#configuration","text":"To enable rdd cache on Intel Optane PMem, you need add the following configurations to spark-defaults.conf spark.memory.pmem.extension.enabled true spark.memory.pmem.initial.path [Your Optane PMem paths seperate with comma] spark.memory.pmem.initial.size [Your Optane PMem size in GB] spark.memory.pmem.usable.ratio [from 0 to 1, 0.85 is recommended] spark.yarn.numa.enabled true spark.yarn.numa.num [Your numa node number] spark.memory.pmem.mode [AppDirect | KMemDax] spark.files file://${PATH_TO_PMEM_SPILL_JAR}/pmem-rdd-cache-<version>-with-spark-<version>.jar,file://${PATH_TO_PMEM_COMMON_JAR}/pmem-common-<version>-with-spark-<version>.jar spark.executor.extraClassPath ./pmem-rdd-cache-<version>-with-spark-<version>.jar:./pmem-common-<version>-with-spark-<version>.jar spark.driver.extraClassPath file://${PATH_TO_PMEM_SPILL_JAR}/pmem-rdd-cache-<version>-with-spark-<version>.jar:file://${PATH_TO_PMEM_COMMON_JAR}/pmem-common-<version>-with-spark-<version>.jar","title":"Configuration"},{"location":"User-Guide/#use-optane-pmem-to-cache-data","text":"There's a new StorageLevel: PMEM_AND_DISK being added to cache data to Optane PMem, at the places you previously cache/persist data to memory, use PMEM_AND_DISK to substitute the previous StorageLevel, data will be cached to Optane PMem. persist(StorageLevel.PMEM_AND_DISK)","title":"Use Optane PMem to cache data"},{"location":"User-Guide/#run-k-means-benchmark","text":"You can use Hibench to run K-means workload: After you Build Hibench, then follow Run SparkBench documentation. Here are some tips besides this documentation you need to notice. Follow the documentation to configure these 4 files: HiBench/conf/hadoop.conf HiBench/conf/hibench.conf HiBench/conf/spark.conf HiBench/conf/workloads/ml/kmeans.conf Note that you need add hibench.kmeans.storage.level PMEM_AND_DISK to kmeans.conf , which can enable both PMem and Disk to cache data. If you completed OAP-Installation-Guide , you also need add the following configs to spark.conf spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib Then you can run the following 2 commands to run K-means workloads: bin/workloads/ml/kmeans/prepare/prepare.sh bin/workloads/ml/kmeans/spark/run.sh Then you can find the log as below: patching args= Parsing conf: /home/wh/HiBench/conf/hadoop.conf Parsing conf: /home/wh/HiBench/conf/hibench.conf Parsing conf: /home/wh/HiBench/conf/spark.conf Parsing conf: /home/wh/HiBench/conf/workloads/ml/kmeans.conf probe sleep jar: /opt/Beaver/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar start ScalaSparkKmeans bench hdfs rm -r: /opt/Beaver/hadoop/bin/hadoop --config /opt/Beaver/hadoop/etc/hadoop fs -rm -r -skipTrash hdfs://vsr219:9000/HiBench/Kmeans/Output rm: `hdfs://vsr219:9000/HiBench/Kmeans/Output': No such file or directory hdfs du -s: /opt/Beaver/hadoop/bin/hadoop --config /opt/Beaver/hadoop/etc/hadoop fs -du -s hdfs://vsr219:9000/HiBench/Kmeans/Input Export env: SPARKBENCH_PROPERTIES_FILES=/home/wh/HiBench/report/kmeans/spark/conf/sparkbench/sparkbench.conf Export env: HADOOP_CONF_DIR=/opt/Beaver/hadoop/etc/hadoop Submit Spark job: /opt/Beaver/spark/bin/spark-submit --properties-file /home/wh/HiBench/report/kmeans/spark/conf/sparkbench/spark.conf --class com.intel.hibench.sparkbench.ml.DenseKMeans --master yarn-client --num-executors 2 --executor-cores 45 --executor-memory 100g /home/wh/HiBench/sparkbench/assembly/target/sparkbench-assembly-8.0-SNAPSHOT-dist.jar -k 10 --numIterations 5 --storageLevel PMEM_AND_DISK hdfs://vsr219:9000/HiBench/Kmeans/Input/samples 20/07/03 09:07:49 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-43459116-f4e3-4fe1-bb29-9bca0afa5286 finish ScalaSparkKmeans bench Open the Spark History Web UI and go to the Storage tab page to verify the cache metrics.","title":"Run K-means benchmark"},{"location":"User-Guide/#limitations","text":"For the scenario that data will exceed the block cache capacity. Memkind 1.9.0 and kernel 4.18 is recommended to avoid the unexpected issue.","title":"Limitations"},{"location":"User-Guide/#how-to-contribute","text":"Currently, PMem Spill packages includes all Spark changed files. MemoryMode.java MemoryManager.scala StorageMemoryPool.scala UnifiedMemoryManager.scala MemoryStore.scala BlockManager.scala StorageLevel.scala TestMemoryManager Please make sure your code change in above source code will not break current function. The files from this package should avoid depending on other OAP module except PMem-Common.","title":"How to contribute"}]}